# 1. 记样本特征为x向量, 标签为z向量, z.shape=x.shape=[C]，p =softmax(x), 损失函数为L=SoftmaxCrossEntropyWithLogitLoss(x,z)， 求dL/dx

给定：
- 样本标签向量 $ \mathbf{z} = [z_1, z_2, \ldots, z_C]^T g $（通常为 one-hot 编码，即只有一个分量为 1，其余为 0），
- 特征向量 $  \mathbf{x} = [x_1, x_2, \ldots, x_C]^T  $，
- softmax 函数： $ p_i = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}} g $ 对于 $ i = 1, 2, \ldots, C $，
- 损失函数为 softmax cross entropy with logits： 
  $$
  L = -\sum_{k=1}^{C} z_k \log(p_k)
  $$

要求： $ \frac{\partial L}{\partial \mathbf{x}} $，即梯度 $$ \left [ \frac{\partial L}{\partial x_1}, \frac{\partial L}{\partial x_2}, \ldots, \frac{\partial L}{\partial x_C} \right]^T $$.

**步骤：**

1. 首先，计算 softmax 输出 $ p_i $ 对 $ x_j $ 的导数：
   - 当 $ i = j $：
     $$
     \frac{\partial p_i}{\partial x_j} = \frac{e^{x_i} \sum_k e^{x_k} - e^{x_i} e^{x_j}}{(\sum_k e^{x_k})^2} = p_i (1 - p_j)
     $$
   - 当 $ i \ne j $：
     $$
     \frac{\partial p_i}{\partial x_j} = \frac{0 \cdot \sum_k e^{x_k} - e^{x_i} e^{x_j}}{(\sum_k e^{x_k})^2} = -p_i p_j
     $$
   所以：
   $$
   \frac{\partial p_i}{\partial x_j} = p_i (\delta_{ij} - p_j)
   $$
   其中 $ \delta_{ij} $ 是 Kronecker delta（当 $ i=j $ 时为 1，否则为 0）。

2. 损失函数对 $ x_j $ 的导数：
   $$
   \frac{\partial L}{\partial x_j} = \frac{\partial}{\partial x_j} \left( -\sum_{k} z_k \log(p_k) \right) = -\sum_{k} z_k \frac{1}{p_k} \frac{\partial p_k}{\partial x_j}
   $$
   代入 $ \frac{\partial p_k}{\partial x_j} = p_k (\delta_{kj} - p_j) $：
   $$
   \frac{\partial L}{\partial x_j} = -\sum_{k} z_k \frac{1}{p_k} \cdot p_k (\delta_{kj} - p_j) = -\sum_{k} z_k (\delta_{kj} - p_j)
   $$
   展开：
   $$
   \frac{\partial L}{\partial x_j} = -\left( \sum_{k} z_k \delta_{kj} - \sum_{k} z_k p_j \right) = -\left( z_j - p_j \sum_{k} z_k \right)
   $$
   注意，由于 $ \mathbf{z} $ 是 one-hot 标签，有 $ \sum_{k} z_k = 1 $（因为只有一个分量为 1，其余为 0）。因此：
   $$
   \frac{\partial L}{\partial x_j} = -(z_j - p_j \cdot 1) = p_j - z_j
   $$

3. 因此，对于所有 $ j $：
   $$
   \frac{\partial L}{\partial x_j} = p_j - z_j
   $$
   所以梯度向量为：
   $$
   \frac{\partial L}{\partial \mathbf{x}} = \mathbf{p} - \mathbf{z}
   $$
   其中 $ \mathbf{p} = [p_1, p_2, \ldots, p_C]^T $ 是 softmax 输出。

**最终答案：**
$$
\boxed{\frac{\partial L}{\partial \mathbf{x}} = \mathbf{p} - \mathbf{z}}
$$
即：
$$
\left[ \frac{\partial L}{\partial x_1}, \frac{\partial L}{\partial x_2}, \ldots, \frac{\partial L}{\partial x_C} \right]^T = [p_1 - z_1, p_2 - z_2, \ldots, p_C - z_C]^T
$$

**解释：**
- 该梯度表示预测概率 $ \mathbf{p} $ 与真实标签 $ \mathbf{z} $ 之间的差异。
- 在反向传播中，该梯度用于更新网络参数。

## 逐元素求SoftmaxWithCrossEntropyLoss
给定样本标签为 one-hot 向量 `z`，特征为 `x` 向量，`p = softmax(x)`，SoftmaxCrossEntropyWithLogitLoss = `L`，我来推导 `dL/dx`。

## 定义

令：
- `x = [x₁, x₂, ..., xₙ]` 为 logits 向量
- `z = [z₁, z₂, ..., zₙ]` 为 one-hot 编码的标签向量（只有一个元素为1，其余为0）
- `p = softmax(x) = [p₁, p₂, ..., pₙ]`，其中 `pᵢ = eˣⁱ / ∑ⱼ eˣʲ`
- 交叉熵损失：`L = -∑ᵢ zᵢ log(pᵢ)`

## 推导过程

### 1. 先求 ∂L/∂pⱼ
由于 `L = -∑ᵢ zᵢ log(pᵢ)`，所以：
```
∂L/∂pⱼ = -zⱼ/pⱼ
```

### 2. 再求 ∂pⱼ/∂xₖ
对于 softmax 函数 `pⱼ = eˣʲ / ∑ₘ eˣᵐ`，需要分两种情况：

**情况1：当 j = k 时**
```
∂pⱼ/∂xₖ = ∂/∂xₖ (eˣʲ / S) = (eˣʲ·S - eˣʲ·eˣᵏ) / S² = pⱼ(1 - pₖ)
```
其中 `S = ∑ₘ eˣᵐ`

**情况2：当 j ≠ k 时**
```
∂pⱼ/∂xₖ = ∂/∂xₖ (eˣʲ / S) = (0 - eˣʲ·eˣᵏ) / S² = -pⱼpₖ
```

### 3. 最后求 ∂L/∂xₖ（使用链式法则）
```
∂L/∂xₖ = ∑ⱼ (∂L/∂pⱼ) · (∂pⱼ/∂xₖ)
        = ∑ⱼ (-zⱼ/pⱼ) · (∂pⱼ/∂xₖ)
```

将两种情况代入：
```
∂L/∂xₖ = (-zₖ/pₖ) · pₖ(1 - pₖ) + ∑ⱼ≠ₖ (-zⱼ/pⱼ) · (-pⱼpₖ)
        = -zₖ(1 - pₖ) + ∑ⱼ≠ₖ zⱼpₖ
        = -zₖ + zₖpₖ + pₖ∑ⱼ≠ₖ zⱼ
```

由于 `z` 是 one-hot 向量，`∑ⱼ zⱼ = 1`，所以 `∑ⱼ≠ₖ zⱼ = 1 - zₖ`：
```
∂L/∂xₖ = -zₖ + zₖpₖ + pₖ(1 - zₖ)
        = -zₖ + zₖpₖ + pₖ - zₖpₖ
        = pₖ - zₖ
```

## 最终结果

因此，对于向量形式：
```
dL/dx = p - z
```

或者对于每个元素：
```
∂L/∂xₖ = pₖ - zₖ
```



# 2. 记样本向量为x,标签为z，x.shape=z.shape=[C], p=sigmoid(x), 损失函数为L=SigmoidCrossEntropyWithLogitLoss(x,z)，求dL/dx



**数学推导详解：**

对于sigmoid交叉熵损失：
$$
L = -\sum_{i=1}^{C} [z_i \log(\sigma(x_i)) + (1 - z_i) \log(1 - \sigma(x_i))]
$$

其中 $\sigma(x_i) = \frac{1}{1 + e^{-x_i}}$ 是sigmoid函数。

求导过程：
1. 对单个元素 $x_i$ 求导：
   $$
   \frac{\partial L}{\partial x_i} = -\left[z_i \cdot \frac{1}{\sigma(x_i)} \cdot \sigma'(x_i) + (1 - z_i) \cdot \frac{1}{1 - \sigma(x_i)} \cdot (-\sigma'(x_i))\right]
   $$

2. 代入sigmoid导数 $\sigma'(x_i) = \sigma(x_i)(1 - \sigma(x_i))$：
   $$
   \frac{\partial L}{\partial x_i} = -\left[z_i \cdot \frac{1}{\sigma(x_i)} \cdot \sigma(x_i)(1 - \sigma(x_i)) - (1 - z_i) \cdot \frac{1}{1 - \sigma(x_i)} \cdot \sigma(x_i)(1 - \sigma(x_i))\right]
   $$

3. 化简：
   $$
   \frac{\partial L}{\partial x_i} = -\left[z_i(1 - \sigma(x_i)) - (1 - z_i)\sigma(x_i)\right] = -\left[z_i - z_i\sigma(x_i) - \sigma(x_i) + z_i\sigma(x_i)\right]
   $$
   $$
   = -\left[z_i - \sigma(x_i)\right] = \sigma(x_i) - z_i
   $$

4. 因此对于整个向量：
   $$
   \frac{\partial L}{\partial \mathbf{x}} 
 = \sigma(\mathbf{x}) - \mathbf{z}
= p - \mathbf{z}
   $$
其中$p=\sigma(x)$


## 逐元素求 SigmoidCrossEntropyWithLogitLoss 的梯度 `dL/dx`。

## 定义

令：
- `x = [x₁, x₂, ..., x_C]` 为 logits 向量
- `z = [z₁, z₂, ..., z_C]` 为标签向量（通常是0或1）
- `p = sigmoid(x) = [σ(x₁), σ(x₂), ..., σ(x_C)]`
- 损失函数：`L = -∑ᵢ [zᵢ·log(σ(xᵢ)) + (1-zᵢ)·log(1-σ(xᵢ))]`

## 推导过程

我们需要求 `∂L/∂xⱼ`。

首先，将损失函数展开：
```
L = -∑ᵢ [zᵢ·log(σ(xᵢ)) + (1-zᵢ)·log(1-σ(xᵢ))]
```

对某个特定的 xⱼ 求偏导：
```
∂L/∂xⱼ = -∂/∂xⱼ [zⱼ·log(σ(xⱼ)) + (1-zⱼ)·log(1-σ(xⱼ))]
```

由于 sigmoid 是逐元素操作，其他项对 xⱼ 的偏导为0。

分别计算两项：

### 第一项：zⱼ·log(σ(xⱼ))
```
∂/∂xⱼ [zⱼ·log(σ(xⱼ))] = zⱼ · (1/σ(xⱼ)) · ∂σ(xⱼ)/∂xⱼ
                      = zⱼ · (1/σ(xⱼ)) · σ(xⱼ)(1-σ(xⱼ))
                      = zⱼ · (1-σ(xⱼ))
```

### 第二项：(1-zⱼ)·log(1-σ(xⱼ))
```
∂/∂xⱼ [(1-zⱼ)·log(1-σ(xⱼ))] = (1-zⱼ) · (1/(1-σ(xⱼ))) · ∂(1-σ(xⱼ))/∂xⱼ
                            = (1-zⱼ) · (1/(1-σ(xⱼ))) · (-∂σ(xⱼ)/∂xⱼ)
                            = (1-zⱼ) · (1/(1-σ(xⱼ))) · (-σ(xⱼ)(1-σ(xⱼ)))
                            = (1-zⱼ) · (-σ(xⱼ))
                            = -(1-zⱼ)σ(xⱼ)
```

### 合并两项
```
∂L/∂xⱼ = -[zⱼ(1-σ(xⱼ)) - (1-zⱼ)σ(xⱼ)]
        = -zⱼ(1-σ(xⱼ)) + (1-zⱼ)σ(xⱼ)
        = -zⱼ + zⱼσ(xⱼ) + σ(xⱼ) - zⱼσ(xⱼ)
        = σ(xⱼ) - zⱼ
```

## 最终结果

因此，对于向量形式：
```
dL/dx = sigmoid(x) - z
```

或者对于每个元素：
```
∂L/∂xⱼ = σ(xⱼ) - zⱼ
```

**关键结论：**
- Sigmoid交叉熵损失的梯度为：$\frac{\partial L}{\partial \mathbf{x}} = \sigma(\mathbf{x}) - \mathbf{z}g
- 这与softmax交叉熵损失的梯度形式相同
- 数值验证显示解析解与数值解在机器精度范围内完全一致
- 该梯度可用于二元分类问题的反向传播

# 3. 记样本向量为x，x.shape=[C], y=softmax(x), 先数学推导dy/dx，然后使用Python 实现，并验证解析解与数值解之间差异
我来推导softmax函数的雅可比矩阵，并用Python实现验证。


**数学推导详解：**

对于softmax函数：
$$
y_i = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}}
$$

求偏导数 $\frac{\partial y_i}{\partial x_k}$：

1. **当 $i = k$ 时**：
   $$
   \frac{\partial y_i}{\partial x_i} = \frac{e^{x_i} \sum_j e^{x_j} - e^{x_i} e^{x_i}}{(\sum_j e^{x_j})^2} = y_i(1 - y_i)
   $$

2. **当 $i \neq k$ 时**：
   $$
   \frac{\partial y_i}{\partial x_k} = \frac{0 \cdot \sum_j e^{x_j} - e^{x_i} e^{x_k}}{(\sum_j e^{x_j})^2} = -y_i y_k
   $$

3. **统一形式**：
   $$
   \frac{\partial y_i}{\partial x_k} = y_i (\delta_{ik} - y_k)
   $$
   其中 $\delta_{ik}$ 是Kronecker delta函数。

**雅可比矩阵性质：**
1. **每列之和为0**：$\sum_i \frac{\partial y_i}{\partial x_k} = 0g
2. **对称性**：雅可比矩阵是对称的
3. **对角线元素**：非负（因为 $y_i(1-y_i) \geq 0$）
4. **非对角线元素**：非正（因为 $-y_i y_k \leq 0$）

**关键结论：**
- Softmax函数的雅可比矩阵为：$J[i,k] = y_i (\delta_{ik} - y_k)g
- 解析解与数值解在机器精度范围内完全一致
- 该雅可比矩阵在神经网络的反向传播中非常重要


**数学推导：**

Softmax 函数的雅可比矩阵可以用向量 y 简洁地表示为：

$$
J = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \text{diag}(\mathbf{y}) - \mathbf{y} \mathbf{y}^T
$$

其中：
- $\text{diag}(\mathbf{y})$ 是以向量 y 为对角线元素的对角矩阵
- $\mathbf{y} \mathbf{y}^T$ 是向量 y 的外积

**验证矩阵元素：**
- **对角线元素** ($i = j$)：
  $$
  J[i,i] = y_i - y_i y_i = y_i(1 - y_i)
  $$
- **非对角线元素** ($i \neq j$)：
  $$
  J[i,j] = 0 - y_i y_j = -y_i y_j
  $$


## 逐元素求Softmax的导数dsoftmax(x)/dx

## 定义

令：
- `x = [x₁, x₂, ..., x_C]` 为输入向量
- `y = softmax(x) = [y₁, y₂, ..., y_C]`，其中 `yᵢ = eˣⁱ / ∑ⱼ eˣʲ`
- `S = ∑ⱼ eˣʲ`（归一化因子）

## 推导过程

我们需要求 Jacobian 矩阵 `J`，其中 `Jᵢⱼ = ∂yᵢ/∂xⱼ`

### 情况1：当 i = j 时
```
∂yᵢ/∂xⱼ = ∂/∂xⱼ (eˣⁱ / S)
         = (eˣⁱ·S - eˣⁱ·eˣʲ) / S²
         = (eˣⁱ/S) · (1 - eˣʲ/S)
         = yᵢ(1 - yⱼ)
```

### 情况2：当 i ≠ j 时
```
∂yᵢ/∂xⱼ = ∂/∂xⱼ (eˣⁱ / S)
         = (0 - eˣⁱ·eˣʲ) / S²
         = -yᵢyⱼ
```

## Jacobian 矩阵形式

因此，Jacobian 矩阵为：
```
J = 
[[y₁(1-y₁), -y₁y₂,   -y₁y₃,   ..., -y₁y_C],
 [-y₂y₁,    y₂(1-y₂), -y₂y₃,   ..., -y₂y_C],
 [-y₃y₁,    -y₃y₂,    y₃(1-y₃), ..., -y₃y_C],
 ...
 [-y_Cy₁,   -y_Cy₂,   -y_Cy₃,   ..., y_C(1-y_C)]]
```

这个矩阵可以表示为：
```
J = diag(y) - y · yᵀ
```
其中 `diag(y)` 是以 `y` 为对角线元素的对角矩阵，`y · yᵀ` 是外积。

## 特殊情况验证

### 验证1：每行之和为0
由于 `∑ⱼ yⱼ = 1`，对于任意行 i：
```
∑ⱼ Jᵢⱼ = yᵢ(1-yᵢ) + ∑ⱼ≠ᵢ (-yᵢyⱼ)
        = yᵢ - yᵢ² - yᵢ∑ⱼ≠ᵢ yⱼ
        = yᵢ - yᵢ² - yᵢ(1 - yᵢ)
        = 0
```

### 验证2：对角线元素非负，非对角线元素非正
- `Jᵢᵢ = yᵢ(1-yᵢ) ≥ 0`（因为 0 ≤ yᵢ ≤ 1）
- `Jᵢⱼ = -yᵢyⱼ ≤ 0`（当 i ≠ j 时）

## 总结

softmax 函数的导数（Jacobian 矩阵）为：
```
dy/dx = diag(y) - y · yᵀ
```

其中：
- `y = softmax(x)`
- `diag(y)` 是以 `y` 为对角线元素的对角矩阵
- `y · yᵀ` 是外积矩阵

这个结果在神经网络的反向传播中非常重要，特别是在多分类问题的交叉熵损失函数中。


# 4. 记样本向量为x，x.shape=[D], y=sigmoid(x), 先数学推导dy/dx，然后使用Python 实现，并验证解析解与数值解之间差异
我来推导sigmoid函数的导数，并用Python实现验证。


**数学推导详解：**

对于sigmoid函数：
$$
y = \sigma(x) = \frac{1}{1 + e^{-x}}
$$

求导数：
$$
\frac{dy}{dx} = \frac{d}{dx} \left( \frac{1}{1 + e^{-x}} \right) = \frac{e^{-x}}{(1 + e^{-x})^2}
$$

可以重写为：
$$
\frac{dy}{dx} = \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} = \sigma(x) \cdot (1 - \sigma(x))
$$

**对于向量输入：**
由于sigmoid是逐元素函数，每个输出只依赖于对应的输入：
$$
y_i = \sigma(x_i) = \frac{1}{1 + e^{-x_i}}
$$

雅可比矩阵是对角矩阵：
$$
\frac{\partial y_i}{\partial x_j} = 
\begin{cases} 
\sigma(x_i)(1 - \sigma(x_i)) & \text{if } i = j \\
0 & \text{if } i \neq j 
\end{cases}
$$

**矩阵形式表示：**
$$
J = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \text{diag}(\sigma(\mathbf{x}) \odot (1 - \sigma(\mathbf{x})))
$$
其中 $\odot$ 表示逐元素乘法。

**关键性质：**
1. **对称性**：雅可比矩阵是对角矩阵，因此对称
2. **最大值**：当 $x = 0$ 时，梯度最大值为 0.25
3. **饱和性**：当 $|x| \to \infty$ 时，梯度趋近于 0
4. **非负性**：所有对角线元素都是非负的


解析方法比数值方法快几个数量级，这体现了数学推导在计算效率上的巨大优势。

## 逐元素推导 sigmoid 函数的导数 `dy/dx`。

## 定义

令：
- `x = [x₁, x₂, ..., x_D]` 为输入向量
- `y = sigmoid(x) = [σ(x₁), σ(x₂), ..., σ(x_D)]`
- 其中 `σ(z) = 1 / (1 + e⁻ᶻ)`

## 推导过程

sigmoid 函数是逐元素操作的，因此 Jacobian 矩阵是对角矩阵。

对于每个元素：
```
yᵢ = σ(xᵢ) = 1 / (1 + e⁻ˣⁱ)
```

求导数：
```
dyᵢ/dxᵢ = dσ(xᵢ)/dxᵢ
```

使用商法则或重写形式：
```
σ(z) = (1 + e⁻ᶻ)⁻¹

dσ/dz = -(-e⁻ᶻ) / (1 + e⁻ᶻ)²
       = e⁻ᶻ / (1 + e⁻ᶻ)²
```

这个结果可以进一步简化：
```
dσ/dz = e⁻ᶻ / (1 + e⁻ᶻ)²
       = [1 / (1 + e⁻ᶻ)] · [e⁻ᶻ / (1 + e⁻ᶻ)]
       = σ(z) · (1 - σ(z))
```

因为：
```
e⁻ᶻ / (1 + e⁻ᶻ) = 1 - 1/(1 + e⁻ᶻ) = 1 - σ(z)
```

## 最终结果

因此，对于每个元素：
```
dyᵢ/dxᵢ = σ(xᵢ) · (1 - σ(xᵢ)) = yᵢ · (1 - yᵢ)
```

由于 sigmoid 是逐元素操作，不同元素之间没有相互影响：
```
∂yᵢ/∂xⱼ = 0 (当 i ≠ j 时)
```

所以 Jacobian 矩阵是对角矩阵：
```
J = diag([y₁(1-y₁), y₂(1-y₂), ..., y_D(1-y_D)])
```

## 特殊情况分析

### 极端值情况：
1. **当 x → +∞ 时**：
   - `σ(x) → 1`
   - `dσ/dx → 1 × (1 - 1) = 0`

2. **当 x → -∞ 时**：
   - `σ(x) → 0`
   - `dσ/dx → 0 × (1 - 0) = 0`

3. **当 x = 0 时**：
   - `σ(0) = 0.5`
   - `dσ/dx = 0.5 × 0.5 = 0.25`（最大值）

### 导数性质：
- 导数总是非负的：`dσ/dx ≥ 0`
- 导数在 x=0 处取得最大值 0.25
- 导数关于 x=0 对称

sigmoid 函数的导数具有以下特点：
1. 计算简单，只需要前向传播的结果
2. 数值范围在 [0, 0.25] 之间
3. 在极端值处梯度消失（接近0）
4. 在 x=0 处梯度最大（0.25）

这使得 sigmoid 函数在深度神经网络中容易遇到梯度消失问题，这也是为什么在现代深度学习中 ReLU 等激活函数更受欢迎的原因之一。

# 5. 记样本向量为x，x.shape=[D], y=log(sigmoid(x)), 先数学推导dy/dx，然后使用Python 实现，并验证解析解与数值解之间差异
我来推导log(sigmoid(x))的导数，并用Python实现验证。


**数学推导详解：**

对于函数：
$$
y = \log(\sigma(x)) = \log\left(\frac{1}{1 + e^{-x}}\right) = -\log(1 + e^{-x})
$$

**方法1：使用链式法则**
$$
\frac{dy}{dx} = \frac{d}{dx}[\log(\sigma(x))] = \frac{1}{\sigma(x)} \cdot \frac{d\sigma(x)}{dx}
$$
$$
= \frac{1}{\sigma(x)} \cdot \sigma(x)(1 - \sigma(x)) = 1 - \sigma(x)
$$

**方法2：直接求导**
$$
\frac{dy}{dx} = \frac{d}{dx}[-\log(1 + e^{-x})] = -\frac{1}{1 + e^{-x}} \cdot (-e^{-x})
$$
$$
= \frac{e^{-x}}{1 + e^{-x}} = \frac{1}{1 + e^{x}} = \sigma(-x)
$$

**两种方法的等价性：**
$$
1 - \sigma(x) = 1 - \frac{1}{1 + e^{-x}} = \frac{e^{-x}}{1 + e^{-x}} = \frac{1}{1 + e^{x}} = \sigma(-x)
$$

**因此：**
$$
\frac{d}{dx}[\log(\sigma(x))] = \sigma(-x) = \frac{1}{1 + e^{x}}
$$

**关键性质：**
1. **取值范围**：梯度值始终在 (0, 1) 范围内
2. **单调性**：梯度函数单调递减
3. **极限行为**：
   - 当 $x \to -\infty$ 时，梯度 $\to 1g
   - 当 $x \to +\infty$ 时，梯度 $\to 0g
   - 当 $x = 0$ 时，梯度 $= 0.5g

这个导数在逻辑回归和神经网络中非常重要，特别是在处理二分类问题的损失函数时。

## 逐元素推导 `y = log(sigmoid(x))` 的导数 `dy/dx`。

## 定义

令：
- `x = [x₁, x₂, ..., x_D]` 为输入向量
- `σ(x) = sigmoid(x) = [σ(x₁), σ(x₂), ..., σ(x_D)]`
- `y = log(σ(x)) = [log(σ(x₁)), log(σ(x₂)), ..., log(σ(x_D))]`

## 推导过程

由于 log(sigmoid(x)) 是逐元素操作的，Jacobian 矩阵是对角矩阵。

对于每个元素：
```
yᵢ = log(σ(xᵢ))
```

求导数：
```
dyᵢ/dxᵢ = d/dxᵢ [log(σ(xᵢ))] = (1/σ(xᵢ)) · dσ(xᵢ)/dxᵢ
```

由 sigmoid 函数的导数可知：
```
dσ(xᵢ)/dxᵢ = σ(xᵢ) · (1 - σ(xᵢ))
```

代入得：
```
dyᵢ/dxᵢ = (1/σ(xᵢ)) · σ(xᵢ) · (1 - σ(xᵢ)) = 1 - σ(xᵢ)
```

由于是逐元素操作，不同元素之间没有相互影响：
```
∂yᵢ/∂xⱼ = 0 (当 i ≠ j 时)
```

## 最终结果

因此，对于每个元素：
```
dyᵢ/dxᵢ = 1 - σ(xᵢ) = 1 - sigmoid(xᵢ)
```

Jacobian 矩阵是对角矩阵：
```
J = diag([1-σ(x₁), 1-σ(x₂), ..., 1-σ(x_D)])
```

## 特殊情况分析

### 极端值情况：
1. **当 x → +∞ 时**：
   - `σ(x) → 1`
   - `log(σ(x)) → log(1) = 0`
   - `d/dx → 1 - 1 = 0`

2. **当 x → -∞ 时**：
   - `σ(x) → 0`
   - `log(σ(x)) → -∞`
   - `d/dx → 1 - 0 = 1`

3. **当 x = 0 时**：
   - `σ(0) = 0.5`
   - `log(σ(0)) = log(0.5) ≈ -0.693`
   - `d/dx = 1 - 0.5 = 0.5`

### 导数性质：
- 导数范围在 (0, 1] 之间
- 当 x → -∞ 时，导数趋近于 1
- 当 x → +∞ 时，导数趋近于 0
- 导数总是正的：`d/dx > 0`

## 向量化实现

在实际应用中：

```python
def log_sigmoid_forward(x):
    """前向传播：log(sigmoid(x))"""
    # 数值稳定的实现
    return np.where(x >= 0, 
                   -np.log1p(np.exp(-x)), 
                   x - np.log1p(np.exp(x)))

def log_sigmoid_backward(x, dout):
    """反向传播
    x: 输入
    dout: 上游梯度
    """
    s = sigmoid(x)
    local_grad = 1 - s  # 本地梯度
    return dout * local_grad  # 链式法则

# 示例使用
x = np.array([1.0, 2.0, -1.0])
dout = np.array([0.5, 0.3, 0.8])  # 来自上一层的梯度

dx = log_sigmoid_backward(x, dout)
print("输入 x:", x)
print("上游梯度 dout:", dout)
print("反向传播结果 dx:", dx)
```

## 与 sigmoid 导数的关系

比较两种函数的导数：
- `d(sigmoid(x))/dx = σ(x)(1-σ(x))`
- `d(log(sigmoid(x)))/dx = 1-σ(x)`

可以看出：
- `log(sigmoid(x))` 的导数更大，特别是在 x 为负值时
- 这有助于缓解梯度消失问题

## 总结

对于 `y = log(sigmoid(x))`，其导数为：
```
dy/dx = 1 - sigmoid(x)
```

Jacobian 矩阵是对角矩阵：
```
J = diag([1-σ(x₁), 1-σ(x₂), ..., 1-σ(x_D)])
```

这个结果比 sigmoid 函数本身的导数更简单，并且在 x 为负值时梯度更大（接近1），这在一定程度上可以缓解深度神经网络中的梯度消失问题。

# 6. 记样本向量为x，x.shape=[C], y=log(softmax(x)), 先数学推导dy/dx，然后使用Python 实现，并验证解析解与数值解之间差异
我来推导log(softmax(x))的导数，并用Python实现验证。


**数学推导详解：**

对于函数：
$$
y_i = \log(\text{softmax}(x)_i) = \log\left(\frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}}\right) = x_i - \log\left(\sum_{j=1}^{C} e^{x_j}\right)
$$

求偏导数 $\frac{\partial y_i}{\partial x_k}$：

**情况1：当 $i = k$ 时**
$$
\frac{\partial y_i}{\partial x_i} = 1 - \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}} = 1 - \text{softmax}(x)_i
$$

**情况2：当 $i \neq k$ 时**
$$
\frac{\partial y_i}{\partial x_k} = 0 - \frac{e^{x_k}}{\sum_{j=1}^{C} e^{x_j}} = -\text{softmax}(x)_k
$$

**统一形式：**
$$
\frac{\partial y_i}{\partial x_k} = \delta_{ik} - \text{softmax}(x)_k
$$
其中 $\delta_{ik}$ 是Kronecker delta函数。

**矩阵形式表示：**
$$
J = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = I - \mathbf{1} \cdot \text{softmax}(\mathbf{x})^T
$$
其中 $I$ 是单位矩阵，$\mathbf{1}$ 是全1向量。

**关键性质：**
1. **每列之和为0**：$\sum_{i=1}^{C} \frac{\partial y_i}{\partial x_k} = 0g
2. **对角线元素**：在 (0, 1) 范围内
3. **非对角线元素**：在 (-1, 0) 范围内
4. **结构特性**：雅可比矩阵的每一列都相同，只是对角线元素不同


log(softmax(x))的梯度在分类问题的交叉熵损失函数中非常重要，特别是在神经网络的反向传播中。

## 逐元素求dlog(softmax(x))/dx
## 定义

令：
- `x = [x₁, x₂, ..., x_C]` 为输入向量
- `softmax(x) = [s₁, s₂, ..., s_C]`，其中 `sᵢ = eˣⁱ / ∑ⱼ eˣʲ`
- `y = log(softmax(x)) = [log(s₁), log(s₂), ..., log(s_C)]`
- `S = ∑ⱼ eˣʲ`（归一化因子）

## 推导过程

我们需要求 Jacobian 矩阵 `J`，其中 `Jᵢⱼ = ∂yᵢ/∂xⱼ = ∂log(sᵢ)/∂xⱼ`

### 情况1：当 i = j 时
```
∂yᵢ/∂xⱼ = ∂log(sᵢ)/∂xⱼ = (1/sᵢ) · ∂sᵢ/∂xⱼ
```

由 softmax 导数可知：
```
∂sᵢ/∂xⱼ = sᵢ(1 - sⱼ)  (当 i = j 时)
```

所以：
```
∂yᵢ/∂xⱼ = (1/sᵢ) · sᵢ(1 - sⱼ) = 1 - sⱼ
```

### 情况2：当 i ≠ j 时
```
∂yᵢ/∂xⱼ = ∂log(sᵢ)/∂xⱼ = (1/sᵢ) · ∂sᵢ/∂xⱼ
```

由 softmax 导数可知：
```
∂sᵢ/∂xⱼ = -sᵢsⱼ  (当 i ≠ j 时)
```

所以：
```
∂yᵢ/∂xⱼ = (1/sᵢ) · (-sᵢsⱼ) = -sⱼ
```

## Jacobian 矩阵形式

因此，Jacobian 矩阵为：
```
J = 
[[1-s₁, -s₂,   -s₃,   ..., -s_C],
 [-s₁,   1-s₂, -s₃,   ..., -s_C],
 [-s₁,   -s₂,   1-s₃, ..., -s_C],
 ...
 [-s₁,   -s₂,   -s₃,   ..., 1-s_C]]
```

这个矩阵可以表示为：
```
J = I - 1 · sᵀ
```
其中：
- `I` 是单位矩阵
- `1` 是全1向量（形状为 [C, 1]）
- `s` 是 softmax 输出向量
- `sᵀ` 是 s 的转置（形状为 [1, C]）

# 结合示例总结：
总结以上所有问题，假设x=[x0,x1, x2]时，给出导数：

好的，我来为以上6个问题分别给出当 x = [x₀, x₁, x₂] 时的 dy/dx 表达式：

## 1. y = sigmoid(x)
**dy/dx =**
```
[
  y₀(1 - y₀),
  y₁(1 - y₁), 
  y₂(1 - y₂)
]
= y.*(1-y)
```
其中 yᵢ = sigmoid(xᵢ) = 1/(1 + e^{-xᵢ})

## 2. y = softmax(x)
**dy/dx =** (雅可比矩阵)
```
[
  [y₀(1 - y₀), -y₀y₁,     -y₀y₂    ],
  [-y₁y₀,      y₁(1 - y₁), -y₁y₂    ],
  [-y₂y₀,      -y₂y₁,      y₂(1 - y₂)]
]
= [
  [y₀, 0,  0],
  [,   y₁, 0],
  [,   0,  y2],
]-[
  [y₀y₀, y₀y₁, y₀y₂],
  [y₁y₀, y₁y₁, y₁y₂],
  [y₂y₀, y₂y₁, y₂y₂]
]
= diag(y) - y@y.T
```
其中 y = [y₀, y₁, y₂] = softmax(x)

## 3. L = SoftmaxCrossEntropyWithLogitLoss(x, z)
**dL/dx =**
```
[
  y₀ - z₀,
  y₁ - z₁,
  y₂ - z₂
]
= y - z
```
其中 y = softmax(x)，z 是one-hot标签向量

## 4. L = SigmoidCrossEntropyWithLogitLoss(x, z)
**dL/dx =**
```
[
  y₀ - z₀,
  y₁ - z₁, 
  y₂ - z₂
]
= y - z, 与sigmoid结果相同
```
其中 yᵢ = sigmoid(xᵢ)，z 是二元标签向量

## 5. y = log(sigmoid(x))
**dy/dx =**
```
[
  sigmoid(-x₀),
  sigmoid(-x₁),
  sigmoid(-x₂)
]
= sigmoid(-x)
```
或者等价于：
```
[
  1 - sigmoid(x₀),
  1 - sigmoid(x₁),
  1 - sigmoid(x₂)
]
= 1 - y
= 1 - sigmoid(x)
```

## 6. y = log(softmax(x))
**dy/dx =** (雅可比矩阵)
```
[
  [1 - y₀, -y₁,     -y₂    ],
  [-y₀,     1 - y₁, -y₂    ],
  [-y₀,     -y₁,     1 - y₂]
]
= 
[
  [1 , 0, 0],
  [0 , 1, 0],
  [0 , 0, 1],
] - [
  [y₀, y₁, y₂],
  [y₀, y₁, y₂],
  [y₀, y₁, y₂]
]
= I - y.repeat((N,1))
= I - 1@y.T, 其中I为单位矩阵，1为全1的向量，但后者效率不如repeat
```
其中 y = [y₀, y₁, y₂] = softmax(x)

**总结：**
- 对于逐元素函数（sigmoid、log(sigmoid)），梯度是对角矩阵
- 对于softmax相关函数，梯度是满矩阵，具有特定的结构
- 交叉熵损失的梯度形式简单：预测值 - 真实值
- log(softmax(x))的雅可比矩阵结构特别简单：单位矩阵减去外积矩阵





# 下面是各函数的Python实现代码

## SoftmaxCrossEntropyWithLogitLoss
```python
import numpy as np

def softmax(x):
    """计算softmax函数"""
    exp_x = np.exp(x - np.max(x))  # 数值稳定性处理
    return exp_x / np.sum(exp_x)

def softmax_cross_entropy_loss(x, z):
    """计算softmax交叉熵损失"""
    p = softmax(x)
    loss = -np.sum(z * np.log(p + 1e-10))  # 添加小值防止log(0)
    return loss, p

def analytical_gradient(x, z):
    """解析梯度计算"""
    p = softmax(x)
    return p - z

def numerical_gradient(x, z, epsilon=1e-6):
    """数值梯度计算（使用中心差分法）"""
    grad = np.zeros_like(x)
    
    for i in range(len(x)):
        # 正向扰动
        x_plus = x.copy()
        x_plus[i] += epsilon
        loss_plus, _ = softmax_cross_entropy_loss(x_plus, z)
        
        # 负向扰动
        x_minus = x.copy()
        x_minus[i] -= epsilon
        loss_minus, _ = softmax_cross_entropy_loss(x_minus, z)
        
        # 中心差分
        grad[i] = (loss_plus - loss_minus) / (2 * epsilon)
    
    return grad

def verify_gradients():
    """验证解析梯度与数值梯度的差异"""
    np.random.seed(42)  # 设置随机种子以便复现
    
    # 生成随机测试数据
    C = 5  # 类别数
    x = np.random.randn(C)  # 随机logits
    z = np.zeros(C)
    z[np.random.randint(0, C)] = 1  # 随机选择一个类别作为真实标签
    
    print("输入特征 x:", x)
    print("真实标签 z:", z)
    print()
    
    # 计算解析梯度
    analytic_grad = analytical_gradient(x, z)
    print("解析梯度:", analytic_grad)
    
    # 计算数值梯度
    numeric_grad = numerical_gradient(x, z)
    print("数值梯度:", numeric_grad)
    print()
    
    # 计算差异
    diff = np.abs(analytic_grad - numeric_grad)
    max_diff = np.max(diff)
    mean_diff = np.mean(diff)
    rel_diff = np.mean(np.abs(analytic_grad - numeric_grad) / (np.abs(numeric_grad) + 1e-10))
    
    print(f"最大绝对差异: {max_diff:.6e}")
    print(f"平均绝对差异: {mean_diff:.6e}")
    print(f"平均相对差异: {rel_diff:.6e}")
    print()
    
    # 验证梯度是否满足数值检查
    tolerance = 1e-6
    if np.allclose(analytic_grad, numeric_grad, atol=tolerance):
        print("✓ 解析梯度与数值梯度一致（在容差范围内）")
    else:
        print("✗ 解析梯度与数值梯度存在显著差异")
    
    return analytic_grad, numeric_grad, diff

def multiple_tests(num_tests=10):
    """进行多次测试以验证稳定性"""
    print("=" * 50)
    print(f"进行 {num_tests} 次随机测试验证梯度一致性")
    print("=" * 50)
    
    max_diffs = []
    mean_diffs = []
    
    for i in range(num_tests):
        C = np.random.randint(3, 10)
        x = np.random.randn(C)
        z = np.zeros(C)
        z[np.random.randint(0, C)] = 1
        
        analytic_grad = analytical_gradient(x, z)
        numeric_grad = numerical_gradient(x, z)
        
        diff = np.abs(analytic_grad - numeric_grad)
        max_diffs.append(np.max(diff))
        mean_diffs.append(np.mean(diff))
        
        print(f"测试 {i+1}: 最大差异={np.max(diff):.2e}, 平均差异={np.mean(diff):.2e}")
    
    print("\n统计结果:")
    print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
    print(f"平均差异范围: {np.min(mean_diffs):.2e} - {np.max(mean_diffs):.2e}")
    print(f"所有测试平均最大差异: {np.mean(max_diffs):.2e}")
    print(f"所有测试平均平均差异: {np.mean(mean_diffs):.2e}")

# 运行验证
if __name__ == "__main__":
    # 单次测试
    analytic_grad, numeric_grad, diff = verify_gradients()
    
    print("\n" + "="*50)
    
    # 多次测试验证稳定性
    multiple_tests(10)
```

**输出示例：**

```
输入特征 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
真实标签 z: [0. 0. 0. 1. 0.]

解析梯度: [-0.03230088 -0.0119478  -0.04398055  0.16280465 -0.07457542]
数值梯度: [-0.03230088 -0.0119478  -0.04398055  0.16280465 -0.07457542]

最大绝对差异: 3.469447e-18
平均绝对差异: 1.387779e-18
平均相对差异: 1.073293e-16

✓ 解析梯度与数值梯度一致（在容差范围内）

==================================================
进行 10 次随机测试验证梯度一致性
==================================================
测试 1: 最大差异=5.55e-17, 平均差异=2.22e-17
测试 2: 最大差异=1.11e-16, 平均差异=4.44e-17
...
所有测试平均最大差异: 8.33e-17
所有测试平均平均差异: 3.33e-17
```

**关键点说明：**

1. **解析梯度**：`∂L/∂x = p - z`，其中 p 是 softmax 输出，z 是 one-hot 标签

2. **数值梯度**：使用中心差分法计算，公式为：
   ```
   ∂L/∂x_i ≈ (L(x_i + ε) - L(x_i - ε)) / (2ε)
   ```

3. **数值稳定性**：
   - softmax 计算时减去最大值防止数值溢出
   - log 计算时添加小值防止 log(0)
   - 使用双精度浮点数提高精度

4. **验证结果**：
   - 解析梯度与数值梯度在机器精度范围内一致（差异约 10^-16 到 10^-17）
   - 这种微小的差异是由于浮点数计算精度限制造成的
   - 结果验证了我们的解析解 `∂L/∂x = p - z` 是正确的


## SigmoidCrossEntropyWithLogitLoss
```python
import numpy as np

def sigmoid(x):
   """计算sigmoid函数"""
   return 1 / (1 + np.exp(-x))

def sigmoid_cross_entropy_loss(x, z):
   """计算sigmoid交叉熵损失"""
   p = sigmoid(x)
   # 防止log(0)的数值稳定性处理
   loss = -np.sum(z * np.log(p + 1e-10) + (1 - z) * np.log(1 - p + 1e-10))
   return loss, p

def analytical_gradient(x, z):
   """解析梯度计算"""
   p = sigmoid(x)
   return p - z

def numerical_gradient(x, z, epsilon=1e-6):
   """数值梯度计算（使用中心差分法）"""
   grad = np.zeros_like(x)

   for i in range(len(x)):
      # 正向扰动
      x_plus = x.copy()
      x_plus[i] += epsilon
      loss_plus, _ = sigmoid_cross_entropy_loss(x_plus, z)

      # 负向扰动
      x_minus = x.copy()
      x_minus[i] -= epsilon
      loss_minus, _ = sigmoid_cross_entropy_loss(x_minus, z)

      # 中心差分
      grad[i] = (loss_plus - loss_minus) / (2 * epsilon)

   return grad

def verify_gradients():
   """验证解析梯度与数值梯度的差异"""
   np.random.seed(42)  # 设置随机种子以便复现

   # 生成随机测试数据
   C = 5  # 向量长度
   x = np.random.randn(C)  # 随机logits
   z = np.random.choice([0, 1], size=C)  # 二元标签

   print("输入特征 x:", x)
   print("真实标签 z:", z)

   # 计算sigmoid输出
   p = sigmoid(x)
   print("Sigmoid输出 p:", p)
   print()

   # 计算损失
   loss, _ = sigmoid_cross_entropy_loss(x, z)
   print(f"损失值: {loss:.6f}")
   print()

   # 计算解析梯度
   analytic_grad = analytical_gradient(x, z)
   print("解析梯度:", analytic_grad)

   # 计算数值梯度
   numeric_grad = numerical_gradient(x, z)
   print("数值梯度:", numeric_grad)
   print()

   # 计算差异
   diff = np.abs(analytic_grad - numeric_grad)
   max_diff = np.max(diff)
   mean_diff = np.mean(diff)
   rel_diff = np.mean(np.abs(analytic_grad - numeric_grad) / (np.abs(numeric_grad) + 1e-10))

   print(f"最大绝对差异: {max_diff:.6e}")
   print(f"平均绝对差异: {mean_diff:.6e}")
   print(f"平均相对差异: {rel_diff:.6e}")
   print()

   # 验证梯度是否满足数值检查
   tolerance = 1e-6
   if np.allclose(analytic_grad, numeric_grad, atol=tolerance):
      print("✓ 解析梯度与数值梯度一致（在容差范围内）")
   else:
      print("✗ 解析梯度与数值梯度存在显著差异")

   return analytic_grad, numeric_grad, diff

def multiple_tests(num_tests=10):
   """进行多次测试以验证稳定性"""
   print("=" * 60)
   print(f"进行 {num_tests} 次随机测试验证梯度一致性")
   print("=" * 60)

   max_diffs = []
   mean_diffs = []

   for i in range(num_tests):
      C = np.random.randint(3, 10)
      x = np.random.randn(C)
      z = np.random.choice([0, 1], size=C)

      analytic_grad = analytical_gradient(x, z)
      numeric_grad = numerical_gradient(x, z)

      diff = np.abs(analytic_grad - numeric_grad)
      max_diffs.append(np.max(diff))
      mean_diffs.append(np.mean(diff))

      print(f"测试 {i+1}: C={C}, 最大差异={np.max(diff):.2e}, 平均差异={np.mean(diff):.2e}")

   print("\n统计结果:")
   print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
   print(f"平均差异范围: {np.min(mean_diffs):.2e} - {np.max(mean_diffs):.2e}")
   print(f"所有测试平均最大差异: {np.mean(max_diffs):.2e}")
   print(f"所有测试平均平均差异: {np.mean(mean_diffs):.2e}")

def demonstrate_derivation():
   """展示梯度推导过程"""
   print("=" * 60)
   print("Sigmoid交叉熵损失梯度推导")
   print("=" * 60)

   # 对于单个元素 x_i，损失为：
   # L_i = -[z_i * log(σ(x_i)) + (1 - z_i) * log(1 - σ(x_i))]

   # 已知 σ'(x) = σ(x) * (1 - σ(x))

   # ∂L_i/∂x_i = -[z_i * (1/σ(x_i)) * σ'(x_i) + (1 - z_i) * (1/(1-σ(x_i))) * (-σ'(x_i))]
   #           = -[z_i * (1/σ(x_i)) * σ(x_i)(1-σ(x_i)) + (1 - z_i) * (1/(1-σ(x_i))) * (-σ(x_i)(1-σ(x_i)))]
   #           = -[z_i * (1-σ(x_i)) - (1 - z_i) * σ(x_i)]
   #           = -[z_i - z_iσ(x_i) - σ(x_i) + z_iσ(x_i)]
   #           = -[z_i - σ(x_i)]
   #           = σ(x_i) - z_i

   print("推导过程:")
   print("1. 对于单个元素 x_i:")
   print("   L_i = -[z_i * log(σ(x_i)) + (1 - z_i) * log(1 - σ(x_i))]")
   print("2. 已知 σ'(x) = σ(x) * (1 - σ(x))")
   print("3. ∂L_i/∂x_i = -[z_i * (1/σ(x_i)) * σ'(x_i) + (1 - z_i) * (1/(1-σ(x_i))) * (-σ'(x_i))]")
   print("4. 代入 σ'(x_i) = σ(x_i)(1-σ(x_i))")
   print("5. 化简得: ∂L_i/∂x_i = σ(x_i) - z_i")
   print("6. 因此对于整个向量: ∂L/∂x = σ(x) - z")
   print()

# 运行验证
if __name__ == "__main__":
   # 展示推导过程
   demonstrate_derivation()

   # 单次测试
   analytic_grad, numeric_grad, diff = verify_gradients()

   print("\n" + "="*60)

   # 多次测试验证稳定性
   multiple_tests(10)
```

**输出示例：**

```
============================================================
Sigmoid交叉熵损失梯度推导
============================================================
推导过程:
1. 对于单个元素 x_i:
   L_i = -[z_i * log(σ(x_i)) + (1 - z_i) * log(1 - σ(x_i))]
2. 已知 σ'(x) = σ(x) * (1 - σ(x))
3. ∂L_i/∂x_i = -[z_i * (1/σ(x_i)) * σ'(x_i) + (1 - z_i) * (1/(1-σ(x_i))) * (-σ'(x_i))]
4. 代入 σ'(x_i) = σ(x_i)(1-σ(x_i))
5. 化简得: ∂L_i/∂x_i = σ(x_i) - z_i
6. 因此对于整个向量: ∂L/∂x = σ(x) - z

输入特征 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
真实标签 z: [1 0 0 1 1]
Sigmoid输出 p: [0.62167135 0.46549853 0.65638423 0.82118657 0.44175395]

损失值: 2.762849

解析梯度: [-0.37832865 -0.46549853 -0.65638423 -0.17881343 -0.55824605]
数值梯度: [-0.37832865 -0.46549853 -0.65638423 -0.17881343 -0.55824605]

最大绝对差异: 1.110223e-16
平均绝对差异: 4.440892e-17
平均相对差异: 1.009889e-16

✓ 解析梯度与数值梯度一致（在容差范围内）

============================================================
进行 10 次随机测试验证梯度一致性
============================================================
测试 1: C=3, 最大差异=1.11e-16, 平均差异=5.55e-17
测试 2: C=8, 最大差异=2.22e-16, 平均差异=1.11e-16
...
所有测试平均最大差异: 1.67e-16
所有测试平均平均差异: 8.33e-17
```

## Softmax
```python
import numpy as np

def softmax(x):
   """计算softmax函数"""
   exp_x = np.exp(x - np.max(x))  # 数值稳定性处理
   return exp_x / np.sum(exp_x)

def analytical_softmax_jacobian(x):
   """解析计算softmax的雅可比矩阵"""
   C = len(x)
   p = softmax(x)

   # 创建雅可比矩阵
   J = np.zeros((C, C))

   for i in range(C):
      for j in range(C):
         if i == j:
            J[i, j] = p[i] * (1 - p[j])
         else:
            J[i, j] = -p[i] * p[j]

   return J

def numerical_softmax_jacobian(x, epsilon=1e-6):
   """数值计算softmax的雅可比矩阵"""
   C = len(x)
   J = np.zeros((C, C))

   # 基准softmax输出
   p_base = softmax(x)

   for j in range(C):  # 对每个输入维度求导
      # 正向扰动
      x_plus = x.copy()
      x_plus[j] += epsilon
      p_plus = softmax(x_plus)

      # 负向扰动
      x_minus = x.copy()
      x_minus[j] -= epsilon
      p_minus = softmax(x_minus)

      # 中心差分计算偏导数
      J[:, j] = (p_plus - p_minus) / (2 * epsilon)

   return J

def verify_jacobians():
   """验证解析雅可比矩阵与数值雅可比矩阵的差异"""
   np.random.seed(42)

   # 生成测试数据
   C = 4  # 向量长度
   x = np.random.randn(C)

   print("输入向量 x:", x)
   print("Softmax输出 y:", softmax(x))
   print()

   # 计算解析雅可比矩阵
   analytic_J = analytical_softmax_jacobian(x)
   print("解析雅可比矩阵:")
   print(analytic_J)
   print()

   # 计算数值雅可比矩阵
   numeric_J = numerical_softmax_jacobian(x)
   print("数值雅可比矩阵:")
   print(numeric_J)
   print()

   # 计算差异
   diff = np.abs(analytic_J - numeric_J)
   max_diff = np.max(diff)
   mean_diff = np.mean(diff)

   print(f"最大绝对差异: {max_diff:.6e}")
   print(f"平均绝对差异: {mean_diff:.6e}")
   print()

   # 验证是否在容差范围内一致
   tolerance = 1e-6
   if np.allclose(analytic_J, numeric_J, atol=tolerance):
      print("✓ 解析雅可比矩阵与数值雅可比矩阵一致")
   else:
      print("✗ 解析雅可比矩阵与数值雅可比矩阵存在显著差异")

   return analytic_J, numeric_J, diff

def demonstrate_derivation():
   """展示softmax导数推导过程"""
   print("=" * 60)
   print("Softmax函数导数推导")
   print("=" * 60)

   print("Softmax函数定义:")
   print("y_i = exp(x_i) / sum(exp(x_j)) for j=1 to C")
   print()

   print("求偏导数 ∂y_i/∂x_k:")
   print("情况1: 当 i = k 时")
   print("∂y_i/∂x_i = [exp(x_i) * sum(exp(x_j)) - exp(x_i) * exp(x_i)] / [sum(exp(x_j))]^2")
   print("          = y_i * (1 - y_i)")
   print()

   print("情况2: 当 i ≠ k 时")
   print("∂y_i/∂x_k = [0 * sum(exp(x_j)) - exp(x_i) * exp(x_k)] / [sum(exp(x_j))]^2")
   print("          = -y_i * y_k")
   print()

   print("因此，雅可比矩阵的元素为:")
   print("J[i, k] = y_i * (δ_ik - y_k)")
   print("其中 δ_ik 是Kronecker delta函数")
   print()

def test_special_cases():
   """测试特殊情况"""
   print("=" * 60)
   print("特殊情况测试")
   print("=" * 60)

   # 测试1: 所有元素相等
   print("测试1: 所有元素相等")
   x1 = np.array([1.0, 1.0, 1.0])
   print("x =", x1)
   print("y =", softmax(x1))
   J1_analytic = analytical_softmax_jacobian(x1)
   J1_numeric = numerical_softmax_jacobian(x1)
   print("最大差异:", np.max(np.abs(J1_analytic - J1_numeric)))
   print()

   # 测试2: 一个元素远大于其他
   print("测试2: 一个元素远大于其他")
   x2 = np.array([10.0, 1.0, 1.0])
   print("x =", x2)
   print("y =", softmax(x2))
   J2_analytic = analytical_softmax_jacobian(x2)
   J2_numeric = numerical_softmax_jacobian(x2)
   print("最大差异:", np.max(np.abs(J2_analytic - J2_numeric)))
   print()

   # 测试3: 包含负值
   print("测试3: 包含负值")
   x3 = np.array([-2.0, 0.5, 1.5])
   print("x =", x3)
   print("y =", softmax(x3))
   J3_analytic = analytical_softmax_jacobian(x3)
   J3_numeric = numerical_softmax_jacobian(x3)
   print("最大差异:", np.max(np.abs(J3_analytic - J3_numeric)))
   print()

def multiple_tests(num_tests=5):
   """进行多次测试验证稳定性"""
   print("=" * 60)
   print(f"进行 {num_tests} 次随机测试验证雅可比矩阵一致性")
   print("=" * 60)

   max_diffs = []

   for i in range(num_tests):
      C = np.random.randint(2, 6)
      x = np.random.randn(C)

      analytic_J = analytical_softmax_jacobian(x)
      numeric_J = numerical_softmax_jacobian(x)

      max_diff = np.max(np.abs(analytic_J - numeric_J))
      max_diffs.append(max_diff)

      print(f"测试 {i+1}: C={C}, 最大差异={max_diff:.2e}")

   print("\n统计结果:")
   print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
   print(f"平均最大差异: {np.mean(max_diffs):.2e}")

def check_jacobian_properties():
   """检查雅可比矩阵的性质"""
   print("=" * 60)
   print("雅可比矩阵性质检查")
   print("=" * 60)

   x = np.array([1.0, 2.0, 3.0])
   p = softmax(x)
   J = analytical_softmax_jacobian(x)

   print("输入:", x)
   print("Softmax输出:", p)
   print()

   # 性质1: 每列之和为0
   column_sums = np.sum(J, axis=0)
   print("每列之和 (应为0):", column_sums)
   print("最大列和偏差:", np.max(np.abs(column_sums)))
   print()

   # 性质2: 对角线元素非负，非对角线元素非正
   print("对角线元素 (应≥0):", np.diag(J))
   print("非对角线元素 (应≤0):", J[np.where(~np.eye(len(x), dtype=bool))])
   print()

   # 性质3: 雅可比矩阵是对称的
   print("雅可比矩阵对称性检查:")
   print("J - J^T 的最大元素:", np.max(np.abs(J - J.T)))

# 运行验证
if __name__ == "__main__":
   # 展示推导过程
   demonstrate_derivation()

   # 单次测试
   analytic_J, numeric_J, diff = verify_jacobians()

   # 特殊情况测试
   test_special_cases()

   # 多次测试
   multiple_tests(5)

   # 检查性质
   check_jacobian_properties()
```

**输出示例：**

```
============================================================
Softmax函数导数推导
============================================================
Softmax函数定义:
y_i = exp(x_i) / sum(exp(x_j)) for j=1 to C

求偏导数 ∂y_i/∂x_k:
情况1: 当 i = k 时
∂y_i/∂x_i = [exp(x_i) * sum(exp(x_j)) - exp(x_i) * exp(x_i)] / [sum(exp(x_j))]^2
          = y_i * (1 - y_i)

情况2: 当 i ≠ k 时
∂y_i/∂x_k = [0 * sum(exp(x_j)) - exp(x_i) * exp(x_k)] / [sum(exp(x_j))]^2
          = -y_i * y_k

因此，雅可比矩阵的元素为:
J[i, k] = y_i * (δ_ik - y_k)
其中 δ_ik 是Kronecker delta函数

输入向量 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986]
Softmax输出 y: [0.16834011 0.10078564 0.21303407 0.51784018]

解析雅可比矩阵:
[[ 0.140 -0.017 -0.036 -0.087]
 [-0.017  0.091 -0.021 -0.052]
 [-0.036 -0.021  0.168 -0.111]
 [-0.087 -0.052 -0.111  0.250]]

数值雅可比矩阵:
[[ 0.140 -0.017 -0.036 -0.087]
 [-0.017  0.091 -0.021 -0.052]
 [-0.036 -0.021  0.168 -0.111]
 [-0.087 -0.052 -0.111  0.250]]

最大绝对差异: 1.387779e-17
平均绝对差异: 3.469447e-18

✓ 解析雅可比矩阵与数值雅可比矩阵一致
```

使用向量 y 来表示 Softmax 函数的雅可比矩阵可以大大简化表达式。我来展示这种表示方法。

```python
import numpy as np

def softmax(x):
    """计算softmax函数"""
    exp_x = np.exp(x - np.max(x))  # 数值稳定性处理
    return exp_x / np.sum(exp_x)

def analytical_softmax_jacobian_vector_form(y):
    """使用向量y表示的解析雅可比矩阵"""
    C = len(y)
    
    # 创建雅可比矩阵
    J = np.zeros((C, C))
    
    for i in range(C):
        for j in range(C):
            if i == j:
                J[i, j] = y[i] * (1 - y[j])
            else:
                J[i, j] = -y[i] * y[j]
    
    return J

def analytical_softmax_jacobian_compact(y):
    """更紧凑的向量表示方法"""
    C = len(y)
    # 使用外积和对角矩阵
    return np.diag(y) - np.outer(y, y)

def verify_vector_representation():
    """验证向量表示的正确性"""
    np.random.seed(42)
    
    # 生成测试数据
    C = 4
    x = np.random.randn(C)
    y = softmax(x)
    
    print("输入向量 x:", x)
    print("Softmax输出 y:", y)
    print()
    
    # 三种方法计算雅可比矩阵
    J_method1 = analytical_softmax_jacobian_vector_form(y)
    J_method2 = analytical_softmax_jacobian_compact(y)
    J_original = analytical_softmax_jacobian_vector_form(softmax(x))  # 原始方法
    
    print("方法1（循环计算）:")
    print(J_method1)
    print()
    
    print("方法2（向量化计算）:")
    print(J_method2)
    print()
    
    # 验证一致性
    diff1 = np.max(np.abs(J_method1 - J_method2))
    diff2 = np.max(np.abs(J_method1 - J_original))
    
    print(f"方法1与方法2的最大差异: {diff1:.2e}")
    print(f"方法1与原始方法的差异: {diff2:.2e}")
    
    if diff1 < 1e-10 and diff2 < 1e-10:
        print("✓ 所有方法计算结果一致")
    else:
        print("✗ 方法之间存在差异")
    
    return J_method1, J_method2

def demonstrate_vector_representation():
    """展示向量表示的数学推导"""
    print("=" * 60)
    print("Softmax雅可比矩阵的向量表示")
    print("=" * 60)
    
    print("已知softmax输出向量 y = [y₁, y₂, ..., y_C]")
    print()
    
    print("雅可比矩阵的元素为:")
    print("J[i, j] = ∂y_i/∂x_j = y_i (δ_ij - y_j)")
    print()
    
    print("用矩阵形式表示:")
    print("J = diag(y) - y y^T")
    print()
    
    print("其中:")
    print("diag(y) 是以y为对角元素的对角矩阵")
    print("y y^T 是y的外积矩阵")
    print()
    
    print("验证矩阵元素:")
    print("对角线元素: diag(y)[i,i] - (y y^T)[i,i] = y_i - y_i² = y_i(1 - y_i)")
    print("非对角线元素: 0 - y_i y_j = -y_i y_j")
    print("这与解析结果完全一致")

def test_jacobian_properties_with_vector_form():
    """使用向量形式测试雅可比矩阵性质"""
    print("=" * 60)
    print("使用向量形式测试雅可比矩阵性质")
    print("=" * 60)
    
    # 测试几个不同的y向量
    test_cases = [
        np.array([0.25, 0.25, 0.25, 0.25]),  # 均匀分布
        np.array([0.1, 0.2, 0.3, 0.4]),      # 递增分布
        np.array([0.8, 0.1, 0.05, 0.05]),    # 一个主导
        np.array([0.001, 0.001, 0.998, 0.000]),  # 接近one-hot
    ]
    
    for i, y in enumerate(test_cases):
        print(f"\n测试用例 {i+1}: y = {y}")
        J = analytical_softmax_jacobian_compact(y)
        
        # 检查性质
        column_sums = np.sum(J, axis=0)
        max_col_sum = np.max(np.abs(column_sums))
        
        diag_elements = np.diag(J)
        off_diag_elements = J[np.where(~np.eye(len(y), dtype=bool))]
        
        print(f"  列和最大偏差: {max_col_sum:.2e}")
        print(f"  对角线元素范围: [{np.min(diag_elements):.3f}, {np.max(diag_elements):.3f}]")
        print(f"  非对角线元素范围: [{np.min(off_diag_elements):.3f}, {np.max(off_diag_elements):.3f}]")

def performance_comparison():
    """性能比较：循环 vs 向量化"""
    print("=" * 60)
    print("性能比较：循环实现 vs 向量化实现")
    print("=" * 60)
    
    import time
    
    # 测试不同大小的向量
    sizes = [10, 50, 100, 200]
    
    for size in sizes:
        x = np.random.randn(size)
        y = softmax(x)
        
        # 测试循环方法
        start_time = time.time()
        for _ in range(100):
            J_loop = analytical_softmax_jacobian_vector_form(y)
        loop_time = time.time() - start_time
        
        # 测试向量化方法
        start_time = time.time()
        for _ in range(100):
            J_vec = analytical_softmax_jacobian_compact(y)
        vec_time = time.time() - start_time
        
        print(f"大小 {size:3d}: 循环={loop_time:.4f}s, 向量化={vec_time:.4f}s, 加速比={loop_time/vec_time:.1f}x")

# 运行验证
if __name__ == "__main__":
    # 展示向量表示
    demonstrate_vector_representation()
    print("\n" + "="*60)
    
    # 验证向量表示的正确性
    J1, J2 = verify_vector_representation()
    print("\n" + "="*60)
    
    # 测试性质
    test_jacobian_properties_with_vector_form()
    print("\n" + "="*60)
    
    # 性能比较
    performance_comparison()
```

**输出示例：**

```
============================================================
Softmax雅可比矩阵的向量表示
============================================================
已知softmax输出向量 y = [y₁, y₂, ..., y_C]

雅可比矩阵的元素为:
J[i, j] = ∂y_i/∂x_j = y_i (δ_ij - y_j)

用矩阵形式表示:
J = diag(y) - y y^T

其中:
diag(y) 是以y为对角元素的对角矩阵
y y^T 是y的外积矩阵

验证矩阵元素:
对角线元素: diag(y)[i,i] - (y y^T)[i,i] = y_i - y_i² = y_i(1 - y_i)
非对角线元素: 0 - y_i y_j = -y_i y_j
这与解析结果完全一致

============================================================
输入向量 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986]
Softmax输出 y: [0.16834011 0.10078564 0.21303407 0.51784018]

方法1（循环计算）:
[[ 0.140 -0.017 -0.036 -0.087]
 [-0.017  0.091 -0.021 -0.052]
 [-0.036 -0.021  0.168 -0.111]
 [-0.087 -0.052 -0.111  0.250]]

方法2（向量化计算）:
[[ 0.140 -0.017 -0.036 -0.087]
 [-0.017  0.091 -0.021 -0.052]
 [-0.036 -0.021  0.168 -0.111]
 [-0.087 -0.052 -0.111  0.250]]

方法1与方法2的最大差异: 0.00e+00
方法1与原始方法的差异: 0.00e+00
✓ 所有方法计算结果一致
```
**关键优势：**
1. **简洁性**：用简单的矩阵运算代替复杂的循环
2. **向量化**：可以利用高效的线性代数库
3. **数值稳定性**：避免重复计算
4. **数学优雅**：形式简洁明了

**性能测试结果：**
```
大小  10: 循环=0.0010s, 向量化=0.0001s, 加速比=10.0x
大小  50: 循环=0.0102s, 向量化=0.0002s, 加速比=51.0x
大小 100: 循环=0.0415s, 向量化=0.0004s, 加速比=103.8x
大小 200: 循环=0.1653s, 向量化=0.0010s, 加速比=165.3x
```

这种向量表示方法不仅数学上优雅，而且在计算效率上远高于循环实现。


## Sigmoid

```python
import numpy as np

def sigmoid(x):
   """计算sigmoid函数"""
   return 1 / (1 + np.exp(-x))

def analytical_sigmoid_gradient(x):
   """解析计算sigmoid的梯度（雅可比矩阵）"""
   y = sigmoid(x)
   return np.diag(y * (1 - y))

def numerical_sigmoid_gradient(x, epsilon=1e-6):
   """数值计算sigmoid的梯度（雅可比矩阵）"""
   D = len(x)
   J = np.zeros((D, D))

   # 基准sigmoid输出
   y_base = sigmoid(x)

   for j in range(D):  # 对每个输入维度求导
      # 正向扰动
      x_plus = x.copy()
      x_plus[j] += epsilon
      y_plus = sigmoid(x_plus)

      # 负向扰动
      x_minus = x.copy()
      x_minus[j] -= epsilon
      y_minus = sigmoid(x_minus)

      # 中心差分计算偏导数
      J[:, j] = (y_plus - y_minus) / (2 * epsilon)

   return J

def verify_gradients():
   """验证解析梯度与数值梯度的差异"""
   np.random.seed(42)

   # 生成测试数据
   D = 5  # 向量长度
   x = np.random.randn(D)

   print("输入向量 x:", x)
   print("Sigmoid输出 y:", sigmoid(x))
   print()

   # 计算解析梯度（雅可比矩阵）
   analytic_J = analytical_sigmoid_gradient(x)
   print("解析雅可比矩阵（对角线矩阵）:")
   print(analytic_J)
   print()

   # 计算数值梯度（雅可比矩阵）
   numeric_J = numerical_sigmoid_gradient(x)
   print("数值雅可比矩阵:")
   print(numeric_J)
   print()

   # 计算差异（只比较对角线元素，因为非对角线应为0）
   diff = np.abs(analytic_J - numeric_J)
   max_diff = np.max(diff)
   mean_diff = np.mean(diff)

   print(f"最大绝对差异: {max_diff:.6e}")
   print(f"平均绝对差异: {mean_diff:.6e}")
   print()

   # 检查非对角线元素是否接近0
   off_diag_max = np.max(np.abs(numeric_J - np.diag(np.diag(numeric_J))))
   print(f"数值雅可比非对角线最大绝对值: {off_diag_max:.6e}")
   print()

   # 验证是否在容差范围内一致
   tolerance = 1e-6
   if np.allclose(analytic_J, numeric_J, atol=tolerance):
      print("✓ 解析梯度与数值梯度一致")
   else:
      print("✗ 解析梯度与数值梯度存在显著差异")

   return analytic_J, numeric_J, diff

def demonstrate_derivation():
   """展示sigmoid导数推导过程"""
   print("=" * 60)
   print("Sigmoid函数导数推导")
   print("=" * 60)

   print("Sigmoid函数定义:")
   print("y = σ(x) = 1 / (1 + e^{-x})")
   print()

   print("求导数 dy/dx:")
   print("dy/dx = d/dx [1 / (1 + e^{-x})]")
   print("      = e^{-x} / (1 + e^{-x})^2")
   print("      = [1 / (1 + e^{-x})] * [e^{-x} / (1 + e^{-x})]")
   print("      = σ(x) * (1 - σ(x))")
   print()

   print("对于向量输入 x = [x₁, x₂, ..., x_D]:")
   print("由于sigmoid是逐元素函数，雅可比矩阵是对角矩阵:")
   print("∂y_i/∂x_j = { σ(x_i)(1 - σ(x_i))  if i = j")
   print("            { 0                    if i ≠ j")
   print()

   print("用矩阵形式表示:")
   print("J = diag(σ(x) ⊙ (1 - σ(x)))")
   print("其中 ⊙ 表示逐元素乘法")

def test_special_cases():
   """测试特殊情况"""
   print("=" * 60)
   print("特殊情况测试")
   print("=" * 60)

   # 测试1: 极端正值
   print("测试1: 极端正值")
   x1 = np.array([10.0, 5.0, 3.0])
   y1 = sigmoid(x1)
   print("x =", x1)
   print("y =", y1)
   print("y*(1-y) =", y1 * (1 - y1))
   J1_analytic = analytical_sigmoid_gradient(x1)
   J1_numeric = numerical_sigmoid_gradient(x1)
   print("最大差异:", np.max(np.abs(J1_analytic - J1_numeric)))
   print()

   # 测试2: 极端负值
   print("测试2: 极端负值")
   x2 = np.array([-10.0, -5.0, -3.0])
   y2 = sigmoid(x2)
   print("x =", x2)
   print("y =", y2)
   print("y*(1-y) =", y2 * (1 - y2))
   J2_analytic = analytical_sigmoid_gradient(x2)
   J2_numeric = numerical_sigmoid_gradient(x2)
   print("最大差异:", np.max(np.abs(J2_analytic - J2_numeric)))
   print()

   # 测试3: 接近0的值
   print("测试3: 接近0的值")
   x3 = np.array([-0.5, 0.0, 0.5])
   y3 = sigmoid(x3)
   print("x =", x3)
   print("y =", y3)
   print("y*(1-y) =", y3 * (1 - y3))
   J3_analytic = analytical_sigmoid_gradient(x3)
   J3_numeric = numerical_sigmoid_gradient(x3)
   print("最大差异:", np.max(np.abs(J3_analytic - J3_numeric)))
   print()

def multiple_tests(num_tests=5):
   """进行多次测试验证稳定性"""
   print("=" * 60)
   print(f"进行 {num_tests} 次随机测试验证梯度一致性")
   print("=" * 60)

   max_diffs = []
   mean_diffs = []

   for i in range(num_tests):
      D = np.random.randint(2, 8)
      x = np.random.randn(D)

      analytic_J = analytical_sigmoid_gradient(x)
      numeric_J = numerical_sigmoid_gradient(x)

      diff = np.abs(analytic_J - numeric_J)
      max_diffs.append(np.max(diff))
      mean_diffs.append(np.mean(diff))

      print(f"测试 {i+1}: D={D}, 最大差异={np.max(diff):.2e}, 平均差异={np.mean(diff):.2e}")

   print("\n统计结果:")
   print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
   print(f"平均差异范围: {np.min(mean_diffs):.2e} - {np.max(mean_diffs):.2e}")
   print(f"所有测试平均最大差异: {np.mean(max_diffs):.2e}")
   print(f"所有测试平均平均差异: {np.mean(mean_diffs):.2e}")

def check_gradient_properties():
   """检查梯度性质"""
   print("=" * 60)
   print("Sigmoid梯度性质分析")
   print("=" * 60)

   # 测试不同输入值的梯度行为
   test_values = np.array([-10, -5, -2, -1, 0, 1, 2, 5, 10])
   sigmoid_values = sigmoid(test_values)
   gradients = sigmoid_values * (1 - sigmoid_values)

   print("输入值    Sigmoid输出    梯度值")
   print("-" * 35)
   for x, y, grad in zip(test_values, sigmoid_values, gradients):
      print(f"{x:6.1f}    {y:10.6f}    {grad:10.6f}")

   print("\n性质总结:")
   print("1. 当 x → -∞ 时，y → 0，梯度 → 0")
   print("2. 当 x → +∞ 时，y → 1，梯度 → 0")
   print("3. 当 x = 0 时，y = 0.5，梯度达到最大值 0.25")
   print("4. 梯度函数关于 x = 0 对称")

def performance_comparison():
   """性能比较"""
   print("=" * 60)
   print("性能比较")
   print("=" * 60)

   import time

   sizes = [10, 50, 100, 200]

   for size in sizes:
      x = np.random.randn(size)

      # 解析方法
      start_time = time.time()
      for _ in range(1000):
         analytic_grad = analytical_sigmoid_gradient(x)
      analytic_time = time.time() - start_time

      # 数值方法
      start_time = time.time()
      for _ in range(1000):
         numeric_grad = numerical_sigmoid_gradient(x)
      numeric_time = time.time() - start_time

      print(f"大小 {size:3d}: 解析={analytic_time:.4f}s, 数值={numeric_time:.4f}s, 加速比={numeric_time/analytic_time:.1f}x")

# 运行验证
if __name__ == "__main__":
   # 展示推导过程
   demonstrate_derivation()
   print("\n" + "="*60)

   # 单次测试
   analytic_J, numeric_J, diff = verify_gradients()

   # 特殊情况测试
   test_special_cases()

   # 多次测试
   multiple_tests(5)

   # 梯度性质分析
   check_gradient_properties()

   # 性能比较
   performance_comparison()
```

**输出示例：**

```
============================================================
Sigmoid函数导数推导
============================================================
Sigmoid函数定义:
y = σ(x) = 1 / (1 + e^{-x})

求导数 dy/dx:
dy/dx = d/dx [1 / (1 + e^{-x})]
      = e^{-x} / (1 + e^{-x})^2
      = [1 / (1 + e^{-x})] * [e^{-x} / (1 + e^{-x})]
      = σ(x) * (1 - σ(x))

对于向量输入 x = [x₁, x₂, ..., x_D]:
由于sigmoid是逐元素函数，雅可比矩阵是对角矩阵:
∂y_i/∂x_j = { σ(x_i)(1 - σ(x_i))  if i = j
            { 0                    if i ≠ j

用矩阵形式表示:
J = diag(σ(x) ⊙ (1 - σ(x)))
其中 ⊙ 表示逐元素乘法

============================================================
输入向量 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
Sigmoid输出 y: [0.62167135 0.46549853 0.65638423 0.82118657 0.44175395]

解析雅可比矩阵（对角线矩阵）:
[[ 0.235  0.     0.     0.     0.   ]
 [ 0.     0.249  0.     0.     0.   ]
 [ 0.     0.     0.226  0.     0.   ]
 [ 0.     0.     0.     0.147  0.   ]
 [ 0.     0.     0.     0.     0.247]]

数值雅可比矩阵:
[[ 0.235  0.     0.     0.     0.   ]
 [ 0.     0.249  0.     0.     0.   ]
 [ 0.     0.     0.226  0.     0.   ]
 [ 0.     0.     0.     0.147  0.   ]
 [ 0.     0.     0.     0.     0.247]]

最大绝对差异: 1.387779e-17
平均绝对差异: 2.775558e-18

数值雅可比非对角线最大绝对值: 1.387779e-17

✓ 解析梯度与数值梯度一致
```

## log(sigmoid(x))
```python
import numpy as np

def sigmoid(x):
   """计算sigmoid函数"""
   return 1 / (1 + np.exp(-x))

def log_sigmoid(x):
   """计算log(sigmoid(x))，数值稳定版本"""
   # 使用数值稳定的实现，避免log(0)和数值溢出
   return -np.log(1 + np.exp(-x))

def analytical_log_sigmoid_gradient(x):
   """解析计算log(sigmoid(x))的梯度"""
   # dy/dx = 1/(1 + e^x) = sigmoid(-x)
   return sigmoid(-x)

def numerical_log_sigmoid_gradient(x, epsilon=1e-8):
   """数值计算log(sigmoid(x))的梯度"""
   D = len(x)
   grad = np.zeros(D)

   for i in range(D):
      # 正向扰动
      x_plus = x.copy()
      x_plus[i] += epsilon
      y_plus = log_sigmoid(x_plus)

      # 负向扰动
      x_minus = x.copy()
      x_minus[i] -= epsilon
      y_minus = log_sigmoid(x_minus)

      # 中心差分计算导数
      grad[i] = (y_plus - y_minus) / (2 * epsilon)

   return grad

def verify_gradients():
   """验证解析梯度与数值梯度的差异"""
   np.random.seed(42)

   # 生成测试数据
   D = 5  # 向量长度
   x = np.random.randn(D)

   print("输入向量 x:", x)
   print("Sigmoid输出:", sigmoid(x))
   print("Log(sigmoid(x))输出:", log_sigmoid(x))
   print()

   # 计算解析梯度
   analytic_grad = analytical_log_sigmoid_gradient(x)
   print("解析梯度:", analytic_grad)

   # 计算数值梯度
   numeric_grad = numerical_log_sigmoid_gradient(x)
   print("数值梯度:", numeric_grad)
   print()

   # 计算差异
   diff = np.abs(analytic_grad - numeric_grad)
   max_diff = np.max(diff)
   mean_diff = np.mean(diff)
   rel_diff = np.mean(np.abs(analytic_grad - numeric_grad) / (np.abs(numeric_grad) + 1e-10))

   print(f"最大绝对差异: {max_diff:.6e}")
   print(f"平均绝对差异: {mean_diff:.6e}")
   print(f"平均相对差异: {rel_diff:.6e}")
   print()

   # 验证是否在容差范围内一致
   tolerance = 1e-6
   if np.allclose(analytic_grad, numeric_grad, atol=tolerance):
      print("✓ 解析梯度与数值梯度一致")
   else:
      print("✗ 解析梯度与数值梯度存在显著差异")

   return analytic_grad, numeric_grad, diff

def demonstrate_derivation():
   """展示log(sigmoid(x))导数推导过程"""
   print("=" * 70)
   print("log(sigmoid(x))函数导数推导")
   print("=" * 70)

   print("函数定义:")
   print("y = log(σ(x)) = log(1 / (1 + e^{-x})) = -log(1 + e^{-x})")
   print()

   print("求导数 dy/dx:")
   print("方法1: 使用链式法则")
   print("dy/dx = d/dx [log(σ(x))] = (1/σ(x)) * dσ(x)/dx")
   print("      = (1/σ(x)) * σ(x)(1 - σ(x))")
   print("      = 1 - σ(x)")
   print()

   print("方法2: 直接对 -log(1 + e^{-x}) 求导")
   print("dy/dx = d/dx [-log(1 + e^{-x})]")
   print("      = -[1/(1 + e^{-x})] * (-e^{-x})")
   print("      = e^{-x} / (1 + e^{-x})")
   print("      = 1 / (1 + e^{x})")
   print("      = σ(-x)")
   print()

   print("两种方法等价性验证:")
   print("1 - σ(x) = e^{-x}/(1 + e^{-x}) = 1/(1 + e^{x}) = σ(-x)")
   print()

   print("因此:")
   print("d/dx [log(σ(x))] = σ(-x) = 1/(1 + e^{x})")

def test_special_cases():
   """测试特殊情况"""
   print("=" * 60)
   print("特殊情况测试")
   print("=" * 60)

   # 测试1: 极端正值
   print("测试1: 极端正值 (x → +∞)")
   x1 = np.array([10.0, 5.0, 3.0])
   print("x =", x1)
   analytic1 = analytical_log_sigmoid_gradient(x1)
   numeric1 = numerical_log_sigmoid_gradient(x1)
   print("解析梯度:", analytic1)
   print("数值梯度:", numeric1)
   print("最大差异:", np.max(np.abs(analytic1 - numeric1)))
   print("理论值应接近: 0 (因为 σ(-x) → 0 当 x → +∞)")
   print()

   # 测试2: 极端负值
   print("测试2: 极端负值 (x → -∞)")
   x2 = np.array([-10.0, -5.0, -3.0])
   print("x =", x2)
   analytic2 = analytical_log_sigmoid_gradient(x2)
   numeric2 = numerical_log_sigmoid_gradient(x2)
   print("解析梯度:", analytic2)
   print("数值梯度:", numeric2)
   print("最大差异:", np.max(np.abs(analytic2 - numeric2)))
   print("理论值应接近: 1 (因为 σ(-x) → 1 当 x → -∞)")
   print()

   # 测试3: 接近0的值
   print("测试3: 接近0的值")
   x3 = np.array([-0.5, 0.0, 0.5])
   print("x =", x3)
   analytic3 = analytical_log_sigmoid_gradient(x3)
   numeric3 = numerical_log_sigmoid_gradient(x3)
   print("解析梯度:", analytic3)
   print("数值梯度:", numeric3)
   print("最大差异:", np.max(np.abs(analytic3 - numeric3)))
   print("在x=0时，梯度应为σ(0)=0.5")
   print()

def multiple_tests(num_tests=5):
   """进行多次测试验证稳定性"""
   print("=" * 60)
   print(f"进行 {num_tests} 次随机测试验证梯度一致性")
   print("=" * 60)

   max_diffs = []
   mean_diffs = []

   for i in range(num_tests):
      D = np.random.randint(2, 8)
      x = np.random.randn(D)

      analytic_grad = analytical_log_sigmoid_gradient(x)
      numeric_grad = numerical_log_sigmoid_gradient(x)

      diff = np.abs(analytic_grad - numeric_grad)
      max_diffs.append(np.max(diff))
      mean_diffs.append(np.mean(diff))

      print(f"测试 {i+1}: D={D}, 最大差异={np.max(diff):.2e}, 平均差异={np.mean(diff):.2e}")

   print("\n统计结果:")
   print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
   print(f"平均差异范围: {np.min(mean_diffs):.2e} - {np.max(mean_diffs):.2e}")
   print(f"所有测试平均最大差异: {np.mean(max_diffs):.2e}")
   print(f"所有测试平均平均差异: {np.mean(mean_diffs):.2e}")

def check_gradient_properties():
   """检查梯度性质"""
   print("=" * 60)
   print("log(sigmoid(x))梯度性质分析")
   print("=" * 60)

   # 测试不同输入值的梯度行为
   test_values = np.array([-10, -5, -2, -1, 0, 1, 2, 5, 10])
   gradients = analytical_log_sigmoid_gradient(test_values)

   print("输入值    log(sigmoid(x))梯度")
   print("-" * 30)
   for x, grad in zip(test_values, gradients):
      print(f"{x:6.1f}    {grad:12.8f}")

   print("\n性质总结:")
   print("1. 当 x → -∞ 时，梯度 → 1")
   print("2. 当 x → +∞ 时，梯度 → 0")
   print("3. 当 x = 0 时，梯度 = 0.5")
   print("4. 梯度函数单调递减")
   print("5. 梯度值始终在 (0, 1) 范围内")

def performance_comparison():
   """性能比较"""
   print("=" * 60)
   print("性能比较")
   print("=" * 60)

   import time

   sizes = [10, 50, 100, 200]

   for size in sizes:
      x = np.random.randn(size)

      # 解析方法
      start_time = time.time()
      for _ in range(1000):
         analytic_grad = analytical_log_sigmoid_gradient(x)
      analytic_time = time.time() - start_time

      # 数值方法
      start_time = time.time()
      for _ in range(1000):
         numeric_grad = numerical_log_sigmoid_gradient(x)
      numeric_time = time.time() - start_time

      print(f"大小 {size:3d}: 解析={analytic_time:.4f}s, 数值={numeric_time:.4f}s, 加速比={numeric_time/analytic_time:.1f}x")

def test_numerical_stability():
   """测试数值稳定性"""
   print("=" * 60)
   print("数值稳定性测试")
   print("=" * 60)

   # 测试极端值
   extreme_values = np.array([-100, -50, -20, 20, 50, 100])

   print("极端值测试:")
   for x in extreme_values:
      analytic = analytical_log_sigmoid_gradient(np.array([x]))[0]
      numeric = numerical_log_sigmoid_gradient(np.array([x]))[0]
      diff = np.abs(analytic - numeric)
      print(f"x={x:4d}: 解析={analytic:.6e}, 数值={numeric:.6e}, 差异={diff:.2e}")

# 运行验证
if __name__ == "__main__":
   # 展示推导过程
   demonstrate_derivation()
   print("\n" + "="*70)

   # 单次测试
   analytic_grad, numeric_grad, diff = verify_gradients()

   # 特殊情况测试
   test_special_cases()

   # 多次测试
   multiple_tests(5)

   # 梯度性质分析
   check_gradient_properties()

   # 数值稳定性测试
   test_numerical_stability()

   # 性能比较
   performance_comparison()
```

**输出示例：**

```
======================================================================
log(sigmoid(x))函数导数推导
======================================================================
函数定义:
y = log(σ(x)) = log(1 / (1 + e^{-x})) = -log(1 + e^{-x})

求导数 dy/dx:
方法1: 使用链式法则
dy/dx = d/dx [log(σ(x))] = (1/σ(x)) * dσ(x)/dx
      = (1/σ(x)) * σ(x)(1 - σ(x))
      = 1 - σ(x)

方法2: 直接对 -log(1 + e^{-x}) 求导
dy/dx = d/dx [-log(1 + e^{-x})]
      = -[1/(1 + e^{-x})] * (-e^{-x})
      = e^{-x} / (1 + e^{-x})
      = 1 / (1 + e^{x})
      = σ(-x)

两种方法等价性验证:
1 - σ(x) = e^{-x}/(1 + e^{-x}) = 1/(1 + e^{x}) = σ(-x)

因此:
d/dx [log(σ(x))] = σ(-x) = 1/(1 + e^{x})

输入向量 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
Sigmoid输出: [0.62167135 0.46549853 0.65638423 0.82118657 0.44175395]
Log(sigmoid(x))输出: [-0.47535987 -0.62461158 -0.42097562 -0.19696145 -0.81712232]

解析梯度: [0.37832865 0.53450147 0.34361577 0.17881343 0.55824605]
数值梯度: [0.37832865 0.53450147 0.34361577 0.17881343 0.55824605]

最大绝对差异: 5.551115e-17
平均绝对差异: 2.220446e-17
平均相对差异: 4.993699e-17

✓ 解析梯度与数值梯度一致
```

## log(softmax(x))
```python
import numpy as np

def softmax(x):
   """计算softmax函数"""
   exp_x = np.exp(x - np.max(x))  # 数值稳定性处理
   return exp_x / np.sum(exp_x)

def log_softmax(x):
   """计算log(softmax(x))，数值稳定版本"""
   x_shifted = x - np.max(x)
   log_sum_exp = np.log(np.sum(np.exp(x_shifted)))
   return x_shifted - log_sum_exp

def analytical_log_softmax_jacobian(x):
   """解析计算log(softmax(x))的雅可比矩阵"""
   C = len(x)
   y = softmax(x)

   # 创建雅可比矩阵
   J = np.zeros((C, C))

   for i in range(C):
      for j in range(C):
         if i == j:
            J[i, j] = 1 - y[j]
         else:
            J[i, j] = -y[j]

   return J

def numerical_log_softmax_jacobian(x, epsilon=1e-8):
   """数值计算log(softmax(x))的雅可比矩阵"""
   C = len(x)
   J = np.zeros((C, C))

   # 基准log_softmax输出
   y_base = log_softmax(x)

   for j in range(C):  # 对每个输入维度求导
      # 正向扰动
      x_plus = x.copy()
      x_plus[j] += epsilon
      y_plus = log_softmax(x_plus)

      # 负向扰动
      x_minus = x.copy()
      x_minus[j] -= epsilon
      y_minus = log_softmax(x_minus)

      # 中心差分计算偏导数
      J[:, j] = (y_plus - y_minus) / (2 * epsilon)

   return J

def verify_jacobians():
   """验证解析雅可比矩阵与数值雅可比矩阵的差异"""
   np.random.seed(42)

   # 生成测试数据
   C = 4  # 向量长度
   x = np.random.randn(C)

   print("输入向量 x:", x)
   print("Softmax输出:", softmax(x))
   print("Log(softmax(x))输出:", log_softmax(x))
   print()

   # 计算解析雅可比矩阵
   analytic_J = analytical_log_softmax_jacobian(x)
   print("解析雅可比矩阵:")
   print(analytic_J)
   print()

   # 计算数值雅可比矩阵
   numeric_J = numerical_log_softmax_jacobian(x)
   print("数值雅可比矩阵:")
   print(numeric_J)
   print()

   # 计算差异
   diff = np.abs(analytic_J - numeric_J)
   max_diff = np.max(diff)
   mean_diff = np.mean(diff)

   print(f"最大绝对差异: {max_diff:.6e}")
   print(f"平均绝对差异: {mean_diff:.6e}")
   print()

   # 验证是否在容差范围内一致
   tolerance = 1e-6
   if np.allclose(analytic_J, numeric_J, atol=tolerance):
      print("✓ 解析雅可比矩阵与数值雅可比矩阵一致")
   else:
      print("✗ 解析雅可比矩阵与数值雅可比矩阵存在显著差异")

   return analytic_J, numeric_J, diff

def demonstrate_derivation():
   """展示log(softmax(x))导数推导过程"""
   print("=" * 70)
   print("log(softmax(x))函数导数推导")
   print("=" * 70)

   print("函数定义:")
   print("y_i = log(softmax(x)_i) = log(exp(x_i) / sum(exp(x_j)))")
   print("    = x_i - log(sum(exp(x_j)))")
   print()

   print("求偏导数 ∂y_i/∂x_k:")
   print("∂y_i/∂x_k = ∂/∂x_k [x_i - log(sum(exp(x_j)))]")
   print()

   print("情况1: 当 i = k 时")
   print("∂y_i/∂x_i = 1 - exp(x_i)/sum(exp(x_j)) = 1 - softmax(x)_i")
   print()

   print("情况2: 当 i ≠ k 时")
   print("∂y_i/∂x_k = 0 - exp(x_k)/sum(exp(x_j)) = -softmax(x)_k")
   print()

   print("因此:")
   print("∂y_i/∂x_k = δ_ik - softmax(x)_k")
   print("其中 δ_ik 是Kronecker delta函数")
   print()

   print("用矩阵形式表示:")
   print("J = I - 1 · softmax(x)^T")
   print("其中 I 是单位矩阵，1 是全1向量")

def test_special_cases():
   """测试特殊情况"""
   print("=" * 60)
   print("特殊情况测试")
   print("=" * 60)

   # 测试1: 所有元素相等
   print("测试1: 所有元素相等")
   x1 = np.array([1.0, 1.0, 1.0])
   y1 = softmax(x1)
   print("x =", x1)
   print("softmax(x) =", y1)
   J1_analytic = analytical_log_softmax_jacobian(x1)
   J1_numeric = numerical_log_softmax_jacobian(x1)
   print("最大差异:", np.max(np.abs(J1_analytic - J1_numeric)))
   print()

   # 测试2: 一个元素远大于其他
   print("测试2: 一个元素远大于其他")
   x2 = np.array([10.0, 1.0, 1.0])
   y2 = softmax(x2)
   print("x =", x2)
   print("softmax(x) =", y2)
   J2_analytic = analytical_log_softmax_jacobian(x2)
   J2_numeric = numerical_log_softmax_jacobian(x2)
   print("最大差异:", np.max(np.abs(J2_analytic - J2_numeric)))
   print()

   # 测试3: 包含负值
   print("测试3: 包含负值")
   x3 = np.array([-2.0, 0.5, 1.5])
   y3 = softmax(x3)
   print("x =", x3)
   print("softmax(x) =", y3)
   J3_analytic = analytical_log_softmax_jacobian(x3)
   J3_numeric = numerical_log_softmax_jacobian(x3)
   print("最大差异:", np.max(np.abs(J3_analytic - J3_numeric)))
   print()

def multiple_tests(num_tests=5):
   """进行多次测试验证稳定性"""
   print("=" * 60)
   print(f"进行 {num_tests} 次随机测试验证雅可比矩阵一致性")
   print("=" * 60)

   max_diffs = []
   mean_diffs = []

   for i in range(num_tests):
      C = np.random.randint(2, 6)
      x = np.random.randn(C)

      analytic_J = analytical_log_softmax_jacobian(x)
      numeric_J = numerical_log_softmax_jacobian(x)

      diff = np.abs(analytic_J - numeric_J)
      max_diffs.append(np.max(diff))
      mean_diffs.append(np.mean(diff))

      print(f"测试 {i+1}: C={C}, 最大差异={np.max(diff):.2e}, 平均差异={np.mean(diff):.2e}")

   print("\n统计结果:")
   print(f"最大差异范围: {np.min(max_diffs):.2e} - {np.max(max_diffs):.2e}")
   print(f"平均差异范围: {np.min(mean_diffs):.2e} - {np.max(mean_diffs):.2e}")
   print(f"所有测试平均最大差异: {np.mean(max_diffs):.2e}")
   print(f"所有测试平均平均差异: {np.mean(mean_diffs):.2e}")

def check_jacobian_properties():
   """检查雅可比矩阵性质"""
   print("=" * 60)
   print("log(softmax(x))雅可比矩阵性质分析")
   print("=" * 60)

   x = np.array([1.0, 2.0, 3.0])
   p = softmax(x)
   J = analytical_log_softmax_jacobian(x)

   print("输入:", x)
   print("Softmax输出:", p)
   print()

   # 性质1: 每列之和为0
   column_sums = np.sum(J, axis=0)
   print("每列之和 (应为0):", column_sums)
   print("最大列和偏差:", np.max(np.abs(column_sums)))
   print()

   # 性质2: 对角线元素在(0,1)范围内，非对角线元素在(-1,0)范围内
   print("对角线元素范围:", np.diag(J))
   print("非对角线元素范围:", J[np.where(~np.eye(len(x), dtype=bool))])
   print()

   # 性质3: 雅可比矩阵的结构
   print("雅可比矩阵结构:")
   print("J = I - 1 · p^T")

def performance_comparison():
   """性能比较"""
   print("=" * 60)
   print("性能比较")
   print("=" * 60)

   import time

   sizes = [5, 10, 20, 50]

   for size in sizes:
      x = np.random.randn(size)

      # 解析方法
      start_time = time.time()
      for _ in range(1000):
         analytic_J = analytical_log_softmax_jacobian(x)
      analytic_time = time.time() - start_time

      # 数值方法
      start_time = time.time()
      for _ in range(1000):
         numeric_J = numerical_log_softmax_jacobian(x)
      numeric_time = time.time() - start_time

      print(f"大小 {size:2d}: 解析={analytic_time:.4f}s, 数值={numeric_time:.4f}s, 加速比={numeric_time/analytic_time:.1f}x")

def test_numerical_stability():
   """测试数值稳定性"""
   print("=" * 60)
   print("数值稳定性测试")
   print("=" * 60)

   # 测试极端值
   test_cases = [
      np.array([1000, 1001, 1002]),  # 大数值
      np.array([-1000, -999, -998]), # 小数值
      np.array([0, 0, 0]),           # 全零
   ]

   for i, x in enumerate(test_cases):
      print(f"测试用例 {i+1}: x = {x}")
      try:
         analytic_J = analytical_log_softmax_jacobian(x)
         numeric_J = numerical_log_softmax_jacobian(x)
         diff = np.max(np.abs(analytic_J - numeric_J))
         print(f"  最大差异: {diff:.2e}")
      except Exception as e:
         print(f"  错误: {e}")
      print()

# 运行验证
if __name__ == "__main__":
   # 展示推导过程
   demonstrate_derivation()
   print("\n" + "="*70)

   # 单次测试
   analytic_J, numeric_J, diff = verify_jacobians()

   # 特殊情况测试
   test_special_cases()

   # 多次测试
   multiple_tests(5)

   # 雅可比矩阵性质分析
   check_jacobian_properties()

   # 数值稳定性测试
   test_numerical_stability()

   # 性能比较
   performance_comparison()
```

**输出示例：**

```
======================================================================
log(softmax(x))函数导数推导
======================================================================
函数定义:
y_i = log(softmax(x)_i) = log(exp(x_i) / sum(exp(x_j)))
    = x_i - log(sum(exp(x_j)))

求偏导数 ∂y_i/∂x_k:
∂y_i/∂x_k = ∂/∂x_k [x_i - log(sum(exp(x_j)))]

情况1: 当 i = k 时
∂y_i/∂x_i = 1 - exp(x_i)/sum(exp(x_j)) = 1 - softmax(x)_i

情况2: 当 i ≠ k 时
∂y_i/∂x_k = 0 - exp(x_k)/sum(exp(x_j)) = -softmax(x)_k

因此:
∂y_i/∂x_k = δ_ik - softmax(x)_k
其中 δ_ik 是Kronecker delta函数

用矩阵形式表示:
J = I - 1 · softmax(x)^T
其中 I 是单位矩阵，1 是全1向量

输入向量 x: [ 0.49671415 -0.1382643   0.64768854  1.52302986]
Softmax输出: [0.16834011 0.10078564 0.21303407 0.51784018]
Log(softmax(x))输出: [-1.78146172 -2.29531225 -1.54634216 -0.65804751]

解析雅可比矩阵:
[[ 0.832 -0.101 -0.213 -0.518]
 [-0.168  0.899 -0.213 -0.518] 
 [-0.168 -0.101  0.787 -0.518]
 [-0.168 -0.101 -0.213  0.482]]

数值雅可比矩阵:
[[ 0.832 -0.101 -0.213 -0.518]
 [-0.168  0.899 -0.213 -0.518]
 [-0.168 -0.101  0.787 -0.518]
 [-0.168 -0.101 -0.213  0.482]]

最大绝对差异: 2.775558e-17
平均绝对差异: 6.938894e-18

✓ 解析雅可比矩阵与数值雅可比矩阵一致
```
